#!/usr/bin/env python3
# Analyse des sessions utilisateur (lecture, nettoyage, enrichissement, agrÃ©gation, export)

import sys
import argparse
import pandas as pd
import os
from datetime import datetime

# ğŸ“ Ajout du chemin racine du projet pour permettre l'import de modules dans /transformations
pipeline_root = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
sys.path.insert(0, pipeline_root)

# ==============================
# ğŸ¯ Lecture des arguments CLI
# ==============================

parser = argparse.ArgumentParser(description="Analyse des sessions web")
parser.add_argument('--input', required=True, help="Fichier CSV des sessions utilisateur")
parser.add_argument('--chunksize', type=int, default=None, help="Taille des chunks Ã  lire (nombre de lignes)")
args = parser.parse_args()
input_path = args.input
chunksize = args.chunksize  # Par dÃ©faut None si non fourni

# ==============================
# ğŸ“¥ VÃ©rification du fichier d'entrÃ©e
# ==============================

if not os.path.exists(input_path):
    print(f"âŒ Fichier introuvable : {input_path}")
    sys.exit(1)

# ==============================
# ğŸ“¦ Imports des fonctions mÃ©tiers
# ==============================

from transformations.data_cleaner import clean_session_data
from transformations.data_enricher import enrich_session_data
from transformations.data_aggregator import aggregate_session_data
from transformations.data_formatter import export_session_data_partitioned

# Liste qui va contenir les DataFrames traitÃ©s si on utilise les chunks
processed_chunks = []

# ==============================
# ğŸ“š Lecture du CSV par chunks (si demandÃ©)
# ==============================

try:
    if chunksize:
        # Lecture et traitement par morceaux (pour les gros fichiers)
        for i, chunk in enumerate(pd.read_csv(input_path, chunksize=chunksize)):
            print(f"ğŸ”¹ Chunk {i+1} lu ({len(chunk)} lignes)")

            # Nettoyage du chunk
            chunk = clean_session_data(chunk)

            # Enrichissement (ajout d'infos dÃ©rivÃ©es ou calculÃ©es)
            chunk = enrich_session_data(chunk, input_path)

            # On stocke le rÃ©sultat pour concatÃ©nation ultÃ©rieure
            processed_chunks.append(chunk)
    else:
        # Lecture directe en mÃ©moire si pas de chunksize spÃ©cifiÃ©
        df = pd.read_csv(input_path)
        df = clean_session_data(df)
        df = enrich_session_data(df, input_path)
        processed_chunks.append(df)

except Exception as e:
    print(f"âŒ Erreur de lecture ou traitement : {e}")
    sys.exit(1)

# ==============================
# ğŸ§© Fusion des morceaux
# ==============================

df_all = pd.concat(processed_chunks, ignore_index=True)

# ==============================
# ğŸ“Š AgrÃ©gation multi-dimensionnelle
# ==============================

# Dimensions possibles sur lesquelles on veut des indicateurs agrÃ©gÃ©s
dimensions = ["device_type", "browser", "referrer", "country", "city", "conversion"]
df_agg = aggregate_session_data(df_all, dimensions)

# ==============================
# ğŸ’¾ Export final partitionnÃ© par date
# ==============================

export_session_data_partitioned(df_agg, input_path)

# ==============================
# âœ… Fin du traitement
# ==============================

print("âœ… Traitement des sessions terminÃ©.")
sys.exit(0)
