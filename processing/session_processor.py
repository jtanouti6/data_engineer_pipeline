#!/usr/bin/env python3
# Analyse des sessions utilisateur (lecture, nettoyage, enrichissement, agrÃ©gation, export)

import sys
import argparse
import pandas as pd
import os

# ğŸ“ Ajout du chemin racine du projet pour permettre l'import de modules dans /transformations
pipeline_root = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
sys.path.insert(0, pipeline_root)

# ==============================
# ğŸ¯ Lecture des arguments CLI
# ==============================
parser = argparse.ArgumentParser(description="Analyse des sessions web")
parser.add_argument('--input', required=True, help="Fichier CSV des sessions utilisateur")
parser.add_argument('--chunksize', type=int, default=None, help="Taille des chunks Ã  lire (nombre de lignes)")
args = parser.parse_args()
input_path = args.input
chunksize = args.chunksize  # None par dÃ©faut

# ==============================
# ğŸ“¥ VÃ©rification du fichier d'entrÃ©e
# ==============================
if not os.path.exists(input_path):
    print(f"âŒ Fichier introuvable : {input_path}")
    sys.exit(1)

# ==============================
# ğŸ“¦ Imports des fonctions mÃ©tiers
# ==============================
from transformations.data_cleaner import clean_session_data
from transformations.data_enricher import enrich_session_data
from transformations.data_aggregator import aggregate_session_data
from transformations.data_formatter import export_session_data_partitioned

processed_chunks = []

# ==============================
# ğŸ“š Lecture du CSV (chunks ou full)
# ==============================
try:
    if chunksize:
        for i, chunk in enumerate(pd.read_csv(input_path, chunksize=chunksize)):
            print(f"ğŸ”¹ Chunk {i+1} lu ({len(chunk)} lignes)")
            chunk = clean_session_data(chunk)
            # âš ï¸ enrich_session_data ne doit plus Ã©crire sur disque
            chunk = enrich_session_data(chunk)
            processed_chunks.append(chunk)
    else:
        df = pd.read_csv(input_path)
        df = clean_session_data(df)
        df = enrich_session_data(df)  # âš ï¸ sans export ici
        processed_chunks.append(df)
except Exception as e:
    print(f"âŒ Erreur de lecture ou traitement : {e}")
    sys.exit(1)

# ==============================
# ğŸ§© Fusion des morceaux
# ==============================
df_all = pd.concat(processed_chunks, ignore_index=True)

# ==============================
# ğŸ’¾ Export enrichi (append unique ici)
# ==============================
enriched_dir = os.path.join(pipeline_root, "data", "processed", "enriched")
os.makedirs(enriched_dir, exist_ok=True)
enriched_path = os.path.join(enriched_dir, "sessions_enriched.csv")

# Ã‰vite dâ€™Ã©craser : append + header si nouveau fichier
write_header = not os.path.exists(enriched_path)
df_all.to_csv(enriched_path, index=False, mode="a", header=write_header)
print(f"ğŸ’¾ DonnÃ©es de session enrichies exportÃ©es (append) : {enriched_path}")

# (Optionnel anti-duplicates si tu relances souvent la pipeline)
# if "session_id" in df_all.columns:
#     df_all = df_all.drop_duplicates(subset=["session_id"])

# ==============================
# ğŸ“Š AgrÃ©gation multi-dimensionnelle
# ==============================
dimensions = ["device_type", "browser", "referrer", "country", "city", "conversion"]
df_agg = aggregate_session_data(df_all, dimensions)

# ==============================
# ğŸ’¾ Export agrÃ©gÃ© partitionnÃ© par date
# ==============================
export_session_data_partitioned(df_agg, input_path)

print("âœ… Traitement des sessions terminÃ©.")
sys.exit(0)
