# ğŸš€ Data Pipeline E-commerce

## ğŸ“Œ Description
Ce projet met en place un **pipeline de donnÃ©es modulaire et industrialisÃ©** pour analyser les performances dâ€™un site e-commerce.  
Il gÃ¨re lâ€™ingestion, la validation qualitÃ©, le traitement, lâ€™enrichissement, lâ€™agrÃ©gation et lâ€™archivage de plusieurs sources hÃ©tÃ©rogÃ¨nes.

## ğŸ“‚ Sources de donnÃ©es
- **Logs API** : fichiers `.json.gz` regroupÃ©s dans une archive `api_logs.zip`
- **Sessions utilisateurs** : fichiers `.csv`
- **Produits** : fichiers `.csv` et `.xlsx`
- **Utilisateurs** : fichiers `.csv` (base clients, premium, etc.)
- **Ventes / business metrics** : fichiers `.csv`

## ğŸ—ï¸ Architecture
```
data/
 â”œâ”€â”€ raw/             # DonnÃ©es brutes (archives, dÃ©pÃ´ts initiaux)
 â”œâ”€â”€ staging/         # DonnÃ©es intermÃ©diaires (aprÃ¨s extraction/dÃ©compression)
 â”œâ”€â”€ processed/       # DonnÃ©es traitÃ©es (enrichies, agrÃ©gÃ©es, jointes)
 â”‚    â”œâ”€â”€ api_logs/
 â”‚    â”œâ”€â”€ sessions/
 â”‚    â”œâ”€â”€ products/
 â”‚    â”œâ”€â”€ sales/
 â”‚    â”œâ”€â”€ users/
 â”‚    â”œâ”€â”€ enriched/
 â”‚    â””â”€â”€ joined/
 â”œâ”€â”€ quality/         # Rapports de validation + dashboard qualitÃ©
 â””â”€â”€ archive/         # Archives compressÃ©es datÃ©es
logs/                 # Journaux dâ€™exÃ©cution
config/               # ParamÃ¨tres YAML/JSON (schemas, rÃ¨gles qualitÃ©)
orchestration/        # Scripts Bash (master + workers)
processing/           # Scripts Python de traitement (1 par source)
transformations/      # Modules Python (cleaner, enricher, aggregator, formatter)
```

## âš™ï¸ FonctionnalitÃ©s principales

### 1. **Orchestration Bash**
- `pipeline_master.sh` : pilote global  
- DÃ©tection dynamique du nombre de CPU et des **workers en parallÃ¨le**  
- Lecture de la configuration via `config/pipeline_config.yaml`

### 2. **ParallÃ©lisme**
- `worker_manager.sh` : exÃ©cute les traitements Python en parallÃ¨le  
- Nombre de workers ajustÃ© automatiquement en fonction du CPU (`nproc - 1`)  
- Chaque worker traite un fichier indÃ©pendant (logs, sessions, produits, â€¦)

### 3. **Traitement par morceaux (chunking)**
- ParamÃ¨tre `chunksize` transmis aux scripts Python  
- Traitement des gros fichiers CSV/JSON par itÃ©ration (`100 000 lignes` par dÃ©faut)  
- Permet dâ€™Ã©viter une surcharge mÃ©moire et dâ€™accÃ©lÃ©rer le flux

### 4. **Validation qualitÃ©**
- `data_validator.py` : vÃ©rifie
  - ComplÃ©tude (taux de valeurs manquantes)
  - Respect du schÃ©ma (`data_schemas.json`)
  - RÃ¨gles mÃ©tier (`business_rules.yaml`)
  - Seuil de qualitÃ© global (`quality_thresholds.yaml`)
- `quality_monitor.sh` : parallÃ©lise la validation sur plusieurs fichiers
- GÃ©nÃ©ration de rapports JSON par fichier

### 5. **Monitoring & alerting**
- `dashboard_gen.py` : gÃ©nÃ¨re un **dashboard HTML** synthÃ©tique  
- `alert_manager.py` : dÃ©clenche une alerte (simulÃ©e email) si Ã©chec qualitÃ©  
- RÃ©sultats sauvegardÃ©s dans `data/quality/`

### 6. **Archivage**
- `archive_processed_data` dans `pipeline_master.sh` :
  - Archive les fichiers traitÃ©s (`processed`, `raw`, `staging`, `quality`)
  - Nommage datÃ© et compressÃ© `.tar.gz`
  - Nettoyage des rÃ©pertoires aprÃ¨s traitement

### 7. **Dockerisation**
- Image Docker avec :
  - Python 3.11 + pandas
  - Bash, jq, yq, unzip, htop, parallel
- `docker-compose.yml` pour monter `data/` et `logs/` en volumes
- User mapping (`UID:GID`) pour gÃ©rer les droits

### 8. **CI/CD (GitHub Actions)**
- Workflow `docker-image.yml` :
  - Build + versioning de lâ€™image Docker
  - Lancement du pipeline en runner self-hosted
  - Sauvegarde des artefacts (logs + dashboard qualitÃ©)
  - Correction automatique des permissions

## ğŸ§ª Exemple dâ€™exÃ©cution
```bash
# Lancer le pipeline complet
bash orchestration/pipeline_master.sh
```

Sortie typique :
```
ğŸ”§ Initialisation de l'environnement...
ğŸ§® DÃ©tection dynamique : 6 workers autorisÃ©s
âš™ï¸  Lancement du traitement avec 6 workers...
âœ… Traitement des sessions terminÃ©.
âœ… Traitement des produits terminÃ©.
âœ… api_logs.zip traitÃ© et marquÃ© .done
ğŸ“ Rapport sauvegardÃ© : validation_report_sessions_20250718.csv.json
ğŸ“Š Dashboard HTML gÃ©nÃ©rÃ© : data/quality/dashboard.html
ğŸ“© Alerte gÃ©nÃ©rÃ©e : quality_alert.txt
ğŸ—œï¸ Archive compressÃ©e : data/archive/archive_20250818_1334.tar.gz
```

## ğŸš€ Performances observÃ©es
- **Volume testÃ©** : ~5 Go de donnÃ©es brutes  
- **Temps total** : ~41 minutes (avec 6 workers CPU et chunking activÃ©)  
- **Optimisations implÃ©mentÃ©es** :
  - ParallÃ©lisme CPU (N-1 cÅ“urs logiques utilisÃ©s)  
  - Lecture par morceaux (`chunksize` dynamique)  
  - DÃ©tection automatique du format JSON (array vs lines)  

---

ğŸ‘‰ Ã€ ce stade, le pipeline est **modulaire, extensible et industrialisÃ©**.  
La prochaine Ã©tape (optionnelle) est lâ€™intÃ©gration **Big Data (HDFS / PySpark / Hive / Delta)** pour passer Ã  lâ€™Ã©chelle.
